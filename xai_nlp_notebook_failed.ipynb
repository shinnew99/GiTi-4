{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from: https://github.com/shinnew99/xai-nlp-notebooks/tree/master\n",
    "\n",
    "# others referenced: https://github.com/jason9693/APEACH\n",
    "\n",
    "# ! git clone https://github.com/smilegate-ai/korean_unsmile_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a89ad10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Archive:  transformers-3.0.2.zip\n",
      "b0892fa0e8df02d683e05e625b3903209bff362d\n",
      "   creating: transformers-3.0.2/\n",
      "   creating: transformers-3.0.2/.circleci/\n",
      "  inflating: transformers-3.0.2/.circleci/config.yml  \n",
      "  inflating: transformers-3.0.2/.circleci/deploy.sh  \n",
      "  inflating: transformers-3.0.2/.coveragerc  \n",
      "   creating: transformers-3.0.2/.github/\n",
      "   creating: transformers-3.0.2/.github/ISSUE_TEMPLATE/\n",
      "  inflating: transformers-3.0.2/.github/ISSUE_TEMPLATE/---new-benchmark.md  \n",
      "  inflating: transformers-3.0.2/.github/ISSUE_TEMPLATE/--new-model-addition.md  \n",
      "  inflating: transformers-3.0.2/.github/ISSUE_TEMPLATE/bug-report.md  \n",
      "  inflating: transformers-3.0.2/.github/ISSUE_TEMPLATE/feature-request.md  \n",
      "  inflating: transformers-3.0.2/.github/ISSUE_TEMPLATE/migration.md  \n",
      "  inflating: transformers-3.0.2/.github/ISSUE_TEMPLATE/question-help.md  \n",
      "  inflating: transformers-3.0.2/.github/stale.yml  \n",
      "   creating: transformers-3.0.2/.github/workflows/\n",
      "  inflating: transformers-3.0.2/.github/workflows/github-push.yml  \n",
      "  inflating: transformers-3.0.2/.github/workflows/github-torch-hub.yml  \n",
      "  inflating: transformers-3.0.2/.github/workflows/self-push.yml  \n",
      "  inflating: transformers-3.0.2/.github/workflows/self-scheduled.yml  \n",
      "  inflating: transformers-3.0.2/.gitignore  \n",
      "  inflating: transformers-3.0.2/CONTRIBUTING.md  \n",
      "  inflating: transformers-3.0.2/LICENSE  \n",
      " extracting: transformers-3.0.2/MANIFEST.in  \n",
      "  inflating: transformers-3.0.2/Makefile  \n",
      "  inflating: transformers-3.0.2/README.md  \n",
      "  inflating: transformers-3.0.2/codecov.yml  \n",
      "  inflating: transformers-3.0.2/deploy_multi_version_doc.sh  \n",
      "   creating: transformers-3.0.2/docker/\n",
      "   creating: transformers-3.0.2/docker/transformers-cpu/\n",
      "  inflating: transformers-3.0.2/docker/transformers-cpu/Dockerfile  \n",
      "   creating: transformers-3.0.2/docker/transformers-gpu/\n",
      "  inflating: transformers-3.0.2/docker/transformers-gpu/Dockerfile  \n",
      "   creating: transformers-3.0.2/docker/transformers-pytorch-cpu/\n",
      "  inflating: transformers-3.0.2/docker/transformers-pytorch-cpu/Dockerfile  \n",
      "   creating: transformers-3.0.2/docker/transformers-pytorch-gpu/\n",
      "  inflating: transformers-3.0.2/docker/transformers-pytorch-gpu/Dockerfile  \n",
      "   creating: transformers-3.0.2/docker/transformers-tensorflow-cpu/\n",
      "  inflating: transformers-3.0.2/docker/transformers-tensorflow-cpu/Dockerfile  \n",
      "   creating: transformers-3.0.2/docker/transformers-tensorflow-gpu/\n",
      "  inflating: transformers-3.0.2/docker/transformers-tensorflow-gpu/Dockerfile  \n",
      "   creating: transformers-3.0.2/docs/\n",
      "  inflating: transformers-3.0.2/docs/Makefile  \n",
      "  inflating: transformers-3.0.2/docs/README.md  \n",
      "   creating: transformers-3.0.2/docs/source/\n",
      "   creating: transformers-3.0.2/docs/source/_static/\n",
      "   creating: transformers-3.0.2/docs/source/_static/css/\n",
      "  inflating: transformers-3.0.2/docs/source/_static/css/Calibre-Light.ttf  \n",
      "  inflating: transformers-3.0.2/docs/source/_static/css/Calibre-Medium.otf  \n",
      "  inflating: transformers-3.0.2/docs/source/_static/css/Calibre-Regular.otf  \n",
      "  inflating: transformers-3.0.2/docs/source/_static/css/Calibre-Thin.otf  \n",
      "  inflating: transformers-3.0.2/docs/source/_static/css/code-snippets.css  \n",
      "  inflating: transformers-3.0.2/docs/source/_static/css/huggingface.css  \n",
      "   creating: transformers-3.0.2/docs/source/_static/js/\n",
      "  inflating: transformers-3.0.2/docs/source/_static/js/custom.js  \n",
      "  inflating: transformers-3.0.2/docs/source/_static/js/huggingface_logo.svg  \n",
      "  inflating: transformers-3.0.2/docs/source/benchmarks.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/bertology.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/conf.py  \n",
      "    linking: transformers-3.0.2/docs/source/contributing.md  -> ../../CONTRIBUTING.md \n",
      "  inflating: transformers-3.0.2/docs/source/converting_tensorflow_models.rst  \n",
      "    linking: transformers-3.0.2/docs/source/examples.md  -> ../../examples/README.md \n",
      "  inflating: transformers-3.0.2/docs/source/favicon.ico  \n",
      "  inflating: transformers-3.0.2/docs/source/glossary.rst  \n",
      "   creating: transformers-3.0.2/docs/source/imgs/\n",
      "  inflating: transformers-3.0.2/docs/source/imgs/local_attention_mask.png  \n",
      "  inflating: transformers-3.0.2/docs/source/imgs/transformers_logo_name.png  \n",
      "  inflating: transformers-3.0.2/docs/source/imgs/warmup_constant_schedule.png  \n",
      "  inflating: transformers-3.0.2/docs/source/imgs/warmup_cosine_hard_restarts_schedule.png  \n",
      "  inflating: transformers-3.0.2/docs/source/imgs/warmup_cosine_schedule.png  \n",
      "  inflating: transformers-3.0.2/docs/source/imgs/warmup_cosine_warm_restarts_schedule.png  \n",
      "  inflating: transformers-3.0.2/docs/source/imgs/warmup_linear_schedule.png  \n",
      "  inflating: transformers-3.0.2/docs/source/index.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/installation.md  \n",
      "   creating: transformers-3.0.2/docs/source/main_classes/\n",
      "  inflating: transformers-3.0.2/docs/source/main_classes/configuration.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/main_classes/model.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/main_classes/optimizer_schedules.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/main_classes/pipelines.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/main_classes/processors.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/main_classes/tokenizer.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/main_classes/trainer.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/migration.md  \n",
      "   creating: transformers-3.0.2/docs/source/model_doc/\n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/albert.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/auto.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/bart.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/bert.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/camembert.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/ctrl.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/dialogpt.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/distilbert.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/electra.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/encoderdecoder.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/flaubert.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/gpt.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/gpt2.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/longformer.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/marian.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/mobilebert.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/reformer.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/retribert.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/roberta.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/t5.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/transformerxl.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/xlm.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/xlmroberta.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_doc/xlnet.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_sharing.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/model_summary.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/multilingual.rst  \n",
      "    linking: transformers-3.0.2/docs/source/notebooks.md  -> ../../notebooks/README.md \n",
      "  inflating: transformers-3.0.2/docs/source/philosophy.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/preprocessing.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/pretrained_models.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/quicktour.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/task_summary.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/tokenizer_summary.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/torchscript.rst  \n",
      "  inflating: transformers-3.0.2/docs/source/training.rst  \n",
      "   creating: transformers-3.0.2/examples/\n",
      "  inflating: transformers-3.0.2/examples/README.md  \n",
      "   creating: transformers-3.0.2/examples/adversarial/\n",
      "  inflating: transformers-3.0.2/examples/adversarial/README.md  \n",
      "  inflating: transformers-3.0.2/examples/adversarial/run_hans.py  \n",
      "  inflating: transformers-3.0.2/examples/adversarial/utils_hans.py  \n",
      "   creating: transformers-3.0.2/examples/benchmarking/\n",
      "  inflating: transformers-3.0.2/examples/benchmarking/plot_csv_file.py  \n",
      "  inflating: transformers-3.0.2/examples/benchmarking/run_benchmark.py  \n",
      "  inflating: transformers-3.0.2/examples/benchmarking/run_benchmark_tf.py  \n",
      "   creating: transformers-3.0.2/examples/bert-loses-patience/\n",
      "  inflating: transformers-3.0.2/examples/bert-loses-patience/README.md  \n",
      "   creating: transformers-3.0.2/examples/bert-loses-patience/pabee/\n",
      " extracting: transformers-3.0.2/examples/bert-loses-patience/pabee/__init__.py  \n",
      "  inflating: transformers-3.0.2/examples/bert-loses-patience/pabee/modeling_pabee_albert.py  \n",
      "  inflating: transformers-3.0.2/examples/bert-loses-patience/pabee/modeling_pabee_bert.py  \n",
      "  inflating: transformers-3.0.2/examples/bert-loses-patience/run_glue_with_pabee.py  \n",
      "  inflating: transformers-3.0.2/examples/bert-loses-patience/test_run_glue_with_pabee.py  \n",
      "   creating: transformers-3.0.2/examples/bertology/\n",
      "  inflating: transformers-3.0.2/examples/bertology/run_bertology.py  \n",
      "   creating: transformers-3.0.2/examples/contrib/\n",
      "  inflating: transformers-3.0.2/examples/contrib/README.md  \n",
      "   creating: transformers-3.0.2/examples/contrib/mm-imdb/\n",
      "  inflating: transformers-3.0.2/examples/contrib/mm-imdb/README.md  \n",
      "  inflating: transformers-3.0.2/examples/contrib/mm-imdb/run_mmimdb.py  \n",
      "  inflating: transformers-3.0.2/examples/contrib/mm-imdb/utils_mmimdb.py  \n",
      "  inflating: transformers-3.0.2/examples/contrib/run_camembert.py  \n",
      "  inflating: transformers-3.0.2/examples/contrib/run_openai_gpt.py  \n",
      "  inflating: transformers-3.0.2/examples/contrib/run_swag.py  \n",
      "  inflating: transformers-3.0.2/examples/contrib/run_transfo_xl.py  \n",
      "   creating: transformers-3.0.2/examples/distillation/\n",
      "  inflating: transformers-3.0.2/examples/distillation/README.md  \n",
      "  inflating: transformers-3.0.2/examples/distillation/distiller.py  \n",
      "  inflating: transformers-3.0.2/examples/distillation/grouped_batch_sampler.py  \n",
      "  inflating: transformers-3.0.2/examples/distillation/lm_seqs_dataset.py  \n",
      "  inflating: transformers-3.0.2/examples/distillation/requirements.txt  \n",
      "  inflating: transformers-3.0.2/examples/distillation/run_squad_w_distillation.py  \n",
      "   creating: transformers-3.0.2/examples/distillation/scripts/\n",
      "  inflating: transformers-3.0.2/examples/distillation/scripts/binarized_data.py  \n",
      "  inflating: transformers-3.0.2/examples/distillation/scripts/extract.py  \n",
      "  inflating: transformers-3.0.2/examples/distillation/scripts/extract_distilbert.py  \n",
      "  inflating: transformers-3.0.2/examples/distillation/scripts/token_counts.py  \n",
      "  inflating: transformers-3.0.2/examples/distillation/train.py  \n",
      "   creating: transformers-3.0.2/examples/distillation/training_configs/\n",
      "  inflating: transformers-3.0.2/examples/distillation/training_configs/distilbert-base-cased.json  \n",
      "  inflating: transformers-3.0.2/examples/distillation/training_configs/distilbert-base-multilingual-cased.json  \n",
      "  inflating: transformers-3.0.2/examples/distillation/training_configs/distilbert-base-uncased.json  \n",
      "  inflating: transformers-3.0.2/examples/distillation/training_configs/distilgpt2.json  \n",
      "  inflating: transformers-3.0.2/examples/distillation/training_configs/distilroberta-base.json  \n",
      "  inflating: transformers-3.0.2/examples/distillation/utils.py  \n",
      "   creating: transformers-3.0.2/examples/language-modeling/\n",
      "  inflating: transformers-3.0.2/examples/language-modeling/README.md  \n",
      "  inflating: transformers-3.0.2/examples/language-modeling/run_language_modeling.py  \n",
      "  inflating: transformers-3.0.2/examples/lightning_base.py  \n",
      "   creating: transformers-3.0.2/examples/longform-qa/\n",
      "  inflating: transformers-3.0.2/examples/longform-qa/README.md  \n",
      "  inflating: transformers-3.0.2/examples/longform-qa/eli5_app.py  \n",
      "  inflating: transformers-3.0.2/examples/longform-qa/eli5_utils.py  \n",
      "   creating: transformers-3.0.2/examples/movement-pruning/\n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/README.md  \n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/Saving_PruneBERT.ipynb  \n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/bertarize.py  \n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/counts_parameters.py  \n",
      "   creating: transformers-3.0.2/examples/movement-pruning/emmental/\n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/emmental/__init__.py  \n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/emmental/configuration_bert_masked.py  \n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/emmental/modeling_bert_masked.py  \n",
      "   creating: transformers-3.0.2/examples/movement-pruning/emmental/modules/\n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/emmental/modules/__init__.py  \n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/emmental/modules/binarizer.py  \n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/emmental/modules/masked_nn.py  \n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/masked_run_glue.py  \n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/masked_run_squad.py  \n",
      "  inflating: transformers-3.0.2/examples/movement-pruning/requirements.txt  \n",
      "   creating: transformers-3.0.2/examples/multiple-choice/\n",
      "  inflating: transformers-3.0.2/examples/multiple-choice/README.md  \n",
      "  inflating: transformers-3.0.2/examples/multiple-choice/run_multiple_choice.py  \n",
      "  inflating: transformers-3.0.2/examples/multiple-choice/run_tf_multiple_choice.py  \n",
      "  inflating: transformers-3.0.2/examples/multiple-choice/utils_multiple_choice.py  \n",
      "   creating: transformers-3.0.2/examples/question-answering/\n",
      "  inflating: transformers-3.0.2/examples/question-answering/README.md  \n",
      "  inflating: transformers-3.0.2/examples/question-answering/run_squad.py  \n",
      "  inflating: transformers-3.0.2/examples/question-answering/run_tf_squad.py  \n",
      "  inflating: transformers-3.0.2/examples/requirements.txt  \n",
      "   creating: transformers-3.0.2/examples/seq2seq/\n",
      "  inflating: transformers-3.0.2/examples/seq2seq/README.md  \n",
      " extracting: transformers-3.0.2/examples/seq2seq/__init__.py  \n",
      "   creating: transformers-3.0.2/examples/seq2seq/bertabs/\n",
      "  inflating: transformers-3.0.2/examples/seq2seq/bertabs/README.md  \n",
      " extracting: transformers-3.0.2/examples/seq2seq/bertabs/__init__.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/bertabs/configuration_bertabs.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/bertabs/convert_bertabs_original_pytorch_checkpoint.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/bertabs/modeling_bertabs.py  \n",
      " extracting: transformers-3.0.2/examples/seq2seq/bertabs/requirements.txt  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/bertabs/run_summarization.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/bertabs/test_utils_summarization.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/bertabs/utils_summarization.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/callbacks.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/distillation.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/finetune.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/finetune.sh  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/finetune_bart_tiny.sh  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/finetune_t5.sh  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/initialization_utils.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/run_distiller.sh  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/run_eval.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/test_seq2seq_examples.py  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/train_distilbart_cnn.sh  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/train_distilbart_xsum.sh  \n",
      "  inflating: transformers-3.0.2/examples/seq2seq/utils.py  \n",
      "  inflating: transformers-3.0.2/examples/test_examples.py  \n",
      "   creating: transformers-3.0.2/examples/text-classification/\n",
      "  inflating: transformers-3.0.2/examples/text-classification/README.md  \n",
      "  inflating: transformers-3.0.2/examples/text-classification/run_glue.py  \n",
      "  inflating: transformers-3.0.2/examples/text-classification/run_pl.sh  \n",
      "  inflating: transformers-3.0.2/examples/text-classification/run_pl_glue.py  \n",
      "  inflating: transformers-3.0.2/examples/text-classification/run_tf_glue.py  \n",
      "  inflating: transformers-3.0.2/examples/text-classification/run_xnli.py  \n",
      "   creating: transformers-3.0.2/examples/text-generation/\n",
      "  inflating: transformers-3.0.2/examples/text-generation/README.md  \n",
      "   creating: transformers-3.0.2/examples/text-generation/pplm/\n",
      "  inflating: transformers-3.0.2/examples/text-generation/pplm/README.md  \n",
      "   creating: transformers-3.0.2/examples/text-generation/pplm/imgs/\n",
      "  inflating: transformers-3.0.2/examples/text-generation/pplm/imgs/headfigure.png  \n",
      "  inflating: transformers-3.0.2/examples/text-generation/pplm/imgs/wooly.png  \n",
      "  inflating: transformers-3.0.2/examples/text-generation/pplm/pplm_classification_head.py  \n",
      "  inflating: transformers-3.0.2/examples/text-generation/pplm/run_pplm.py  \n",
      "  inflating: transformers-3.0.2/examples/text-generation/pplm/run_pplm_discrim_train.py  \n",
      "  inflating: transformers-3.0.2/examples/text-generation/run_generation.py  \n",
      "   creating: transformers-3.0.2/examples/token-classification/\n",
      "  inflating: transformers-3.0.2/examples/token-classification/README.md  \n",
      "  inflating: transformers-3.0.2/examples/token-classification/run.sh  \n",
      "  inflating: transformers-3.0.2/examples/token-classification/run_ner.py  \n",
      "  inflating: transformers-3.0.2/examples/token-classification/run_pl.sh  \n",
      "  inflating: transformers-3.0.2/examples/token-classification/run_pl_ner.py  \n",
      "  inflating: transformers-3.0.2/examples/token-classification/run_tf_ner.py  \n",
      "   creating: transformers-3.0.2/examples/token-classification/scripts/\n",
      "  inflating: transformers-3.0.2/examples/token-classification/scripts/preprocess.py  \n",
      "  inflating: transformers-3.0.2/examples/token-classification/test_ner_examples.py  \n",
      "  inflating: transformers-3.0.2/examples/token-classification/utils_ner.py  \n",
      "  inflating: transformers-3.0.2/examples/xla_spawn.py  \n",
      "  inflating: transformers-3.0.2/hubconf.py  \n",
      "   creating: transformers-3.0.2/model_cards/\n",
      "   creating: transformers-3.0.2/model_cards/DeepPavlov/\n",
      "   creating: transformers-3.0.2/model_cards/DeepPavlov/bert-base-bg-cs-pl-ru-cased/\n",
      "  inflating: transformers-3.0.2/model_cards/DeepPavlov/bert-base-bg-cs-pl-ru-cased/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/DeepPavlov/bert-base-cased-conversational/\n",
      "  inflating: transformers-3.0.2/model_cards/DeepPavlov/bert-base-cased-conversational/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/DeepPavlov/bert-base-multilingual-cased-sentence/\n",
      "  inflating: transformers-3.0.2/model_cards/DeepPavlov/bert-base-multilingual-cased-sentence/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/DeepPavlov/rubert-base-cased-conversational/\n",
      "  inflating: transformers-3.0.2/model_cards/DeepPavlov/rubert-base-cased-conversational/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/DeepPavlov/rubert-base-cased-sentence/\n",
      "  inflating: transformers-3.0.2/model_cards/DeepPavlov/rubert-base-cased-sentence/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/DeepPavlov/rubert-base-cased/\n",
      "  inflating: transformers-3.0.2/model_cards/DeepPavlov/rubert-base-cased/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Hate-speech-CNERG/\n",
      "   creating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-arabic/\n",
      "  inflating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-arabic/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-english/\n",
      "  inflating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-english/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-french/\n",
      "  inflating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-french/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-german/\n",
      "  inflating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-german/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-indonesian/\n",
      "  inflating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-indonesian/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-italian/\n",
      "  inflating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-italian/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-polish/\n",
      "  inflating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-polish/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-portugese/\n",
      "  inflating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-portugese/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-spanish/\n",
      "  inflating: transformers-3.0.2/model_cards/Hate-speech-CNERG/dehatebert-mono-spanish/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/HooshvareLab/\n",
      "   creating: transformers-3.0.2/model_cards/HooshvareLab/bert-base-parsbert-armanner-uncased/\n",
      "  inflating: transformers-3.0.2/model_cards/HooshvareLab/bert-base-parsbert-armanner-uncased/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/HooshvareLab/bert-base-parsbert-ner-uncased/\n",
      "  inflating: transformers-3.0.2/model_cards/HooshvareLab/bert-base-parsbert-ner-uncased/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/HooshvareLab/bert-base-parsbert-peymaner-uncased/\n",
      "  inflating: transformers-3.0.2/model_cards/HooshvareLab/bert-base-parsbert-peymaner-uncased/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/HooshvareLab/bert-base-parsbert-uncased/\n",
      "  inflating: transformers-3.0.2/model_cards/HooshvareLab/bert-base-parsbert-uncased/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/KB/\n",
      "   creating: transformers-3.0.2/model_cards/KB/albert-base-swedish-cased-alpha/\n",
      "  inflating: transformers-3.0.2/model_cards/KB/albert-base-swedish-cased-alpha/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/KB/bert-base-swedish-cased-ner/\n",
      "  inflating: transformers-3.0.2/model_cards/KB/bert-base-swedish-cased-ner/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/KB/bert-base-swedish-cased/\n",
      "  inflating: transformers-3.0.2/model_cards/KB/bert-base-swedish-cased/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/LorenzoDeMattei/\n",
      "   creating: transformers-3.0.2/model_cards/LorenzoDeMattei/GePpeTto/\n",
      "  inflating: transformers-3.0.2/model_cards/LorenzoDeMattei/GePpeTto/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/MoseliMotsoehli/\n",
      "   creating: transformers-3.0.2/model_cards/MoseliMotsoehli/TswanaBert/\n",
      "  inflating: transformers-3.0.2/model_cards/MoseliMotsoehli/TswanaBert/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Musixmatch/\n",
      "   creating: transformers-3.0.2/model_cards/Musixmatch/umberto-commoncrawl-cased-v1/\n",
      "  inflating: transformers-3.0.2/model_cards/Musixmatch/umberto-commoncrawl-cased-v1/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Musixmatch/umberto-wikipedia-uncased-v1/\n",
      "  inflating: transformers-3.0.2/model_cards/Musixmatch/umberto-wikipedia-uncased-v1/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/NLP4H/\n",
      "   creating: transformers-3.0.2/model_cards/NLP4H/ms_bert/\n",
      "  inflating: transformers-3.0.2/model_cards/NLP4H/ms_bert/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/NeuML/\n",
      "   creating: transformers-3.0.2/model_cards/NeuML/bert-small-cord19-squad2/\n",
      "  inflating: transformers-3.0.2/model_cards/NeuML/bert-small-cord19-squad2/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/NeuML/bert-small-cord19/\n",
      "  inflating: transformers-3.0.2/model_cards/NeuML/bert-small-cord19/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/NeuML/bert-small-cord19qa/\n",
      "  inflating: transformers-3.0.2/model_cards/NeuML/bert-small-cord19qa/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/SparkBeyond/\n",
      "   creating: transformers-3.0.2/model_cards/SparkBeyond/roberta-large-sts-b/\n",
      "  inflating: transformers-3.0.2/model_cards/SparkBeyond/roberta-large-sts-b/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/Tereveni-AI/\n",
      "   creating: transformers-3.0.2/model_cards/Tereveni-AI/gpt2-124M-uk-fiction/\n",
      "  inflating: transformers-3.0.2/model_cards/Tereveni-AI/gpt2-124M-uk-fiction/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/TurkuNLP/\n",
      "   creating: transformers-3.0.2/model_cards/TurkuNLP/bert-base-finnish-cased-v1/\n",
      "  inflating: transformers-3.0.2/model_cards/TurkuNLP/bert-base-finnish-cased-v1/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/TurkuNLP/bert-base-finnish-uncased-v1/\n",
      "  inflating: transformers-3.0.2/model_cards/TurkuNLP/bert-base-finnish-uncased-v1/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/ViktorAlm/\n",
      "   creating: transformers-3.0.2/model_cards/ViktorAlm/electra-base-norwegian-uncased-discriminator/\n",
      "  inflating: transformers-3.0.2/model_cards/ViktorAlm/electra-base-norwegian-uncased-discriminator/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/a-ware/\n",
      "   creating: transformers-3.0.2/model_cards/a-ware/bart-squadv2/\n",
      "  inflating: transformers-3.0.2/model_cards/a-ware/bart-squadv2/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/a-ware/roberta-large-squad-classification/\n",
      "  inflating: transformers-3.0.2/model_cards/a-ware/roberta-large-squad-classification/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/a-ware/xlmroberta-squadv2/\n",
      "  inflating: transformers-3.0.2/model_cards/a-ware/xlmroberta-squadv2/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/activebus/\n",
      "   creating: transformers-3.0.2/model_cards/activebus/BERT-DK_laptop/\n",
      "  inflating: transformers-3.0.2/model_cards/activebus/BERT-DK_laptop/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/activebus/BERT-DK_rest/\n",
      "  inflating: transformers-3.0.2/model_cards/activebus/BERT-DK_rest/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/activebus/BERT-PT_laptop/\n",
      "  inflating: transformers-3.0.2/model_cards/activebus/BERT-PT_laptop/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/activebus/BERT-PT_rest/\n",
      "  inflating: transformers-3.0.2/model_cards/activebus/BERT-PT_rest/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/activebus/BERT-XD_Review/\n",
      "  inflating: transformers-3.0.2/model_cards/activebus/BERT-XD_Review/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/activebus/BERT_Review/\n",
      "  inflating: transformers-3.0.2/model_cards/activebus/BERT_Review/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/ahotrod/\n",
      "   creating: transformers-3.0.2/model_cards/ahotrod/albert_xxlargev1_squad2_512/\n",
      "  inflating: transformers-3.0.2/model_cards/ahotrod/albert_xxlargev1_squad2_512/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/ahotrod/electra_large_discriminator_squad2_512/\n",
      "  inflating: transformers-3.0.2/model_cards/ahotrod/electra_large_discriminator_squad2_512/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/ahotrod/roberta_large_squad2/\n",
      "  inflating: transformers-3.0.2/model_cards/ahotrod/roberta_large_squad2/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/ahotrod/xlnet_large_squad2_512/\n",
      "  inflating: transformers-3.0.2/model_cards/ahotrod/xlnet_large_squad2_512/README.md  \n",
      "  inflating: transformers-3.0.2/model_cards/albert-base-v1-README.md  \n",
      "  inflating: transformers-3.0.2/model_cards/albert-xxlarge-v2-README.md  \n",
      "   creating: transformers-3.0.2/model_cards/allegro/\n",
      "   creating: transformers-3.0.2/model_cards/allegro/herbert-klej-cased-tokenizer-v1/\n",
      "  inflating: transformers-3.0.2/model_cards/allegro/herbert-klej-cased-tokenizer-v1/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/allegro/herbert-klej-cased-v1/\n",
      "  inflating: transformers-3.0.2/model_cards/allegro/herbert-klej-cased-v1/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/allenai/\n",
      "   creating: transformers-3.0.2/model_cards/allenai/biomed_roberta_base/\n",
      "  inflating: transformers-3.0.2/model_cards/allenai/biomed_roberta_base/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/allenai/longformer-base-4096-extra.pos.embd.only/\n",
      "  inflating: transformers-3.0.2/model_cards/allenai/longformer-base-4096-extra.pos.embd.only/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/allenai/longformer-base-4096/\n",
      "  inflating: transformers-3.0.2/model_cards/allenai/longformer-base-4096/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/allenai/scibert_scivocab_cased/\n",
      "  inflating: transformers-3.0.2/model_cards/allenai/scibert_scivocab_cased/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/allenai/scibert_scivocab_uncased/\n",
      "  inflating: transformers-3.0.2/model_cards/allenai/scibert_scivocab_uncased/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/aodiniz/\n",
      "   creating: transformers-3.0.2/model_cards/aodiniz/bert_uncased_L-10_H-512_A-8_cord19-200616/\n",
      "  inflating: transformers-3.0.2/model_cards/aodiniz/bert_uncased_L-10_H-512_A-8_cord19-200616/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/aodiniz/bert_uncased_L-10_H-512_A-8_cord19-200616_squad2/\n",
      "  inflating: transformers-3.0.2/model_cards/aodiniz/bert_uncased_L-10_H-512_A-8_cord19-200616_squad2/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/aodiniz/bert_uncased_L-2_H-512_A-8_cord19-200616/\n",
      "  inflating: transformers-3.0.2/model_cards/aodiniz/bert_uncased_L-2_H-512_A-8_cord19-200616/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/aodiniz/bert_uncased_L-4_H-256_A-4_cord19-200616/\n",
      "  inflating: transformers-3.0.2/model_cards/aodiniz/bert_uncased_L-4_H-256_A-4_cord19-200616/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/asafaya/\n",
      "   creating: transformers-3.0.2/model_cards/asafaya/bert-base-arabic/\n",
      "  inflating: transformers-3.0.2/model_cards/asafaya/bert-base-arabic/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/asafaya/bert-large-arabic/\n",
      "  inflating: transformers-3.0.2/model_cards/asafaya/bert-large-arabic/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/asafaya/bert-medium-arabic/\n",
      "  inflating: transformers-3.0.2/model_cards/asafaya/bert-medium-arabic/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/asafaya/bert-mini-arabic/\n",
      "  inflating: transformers-3.0.2/model_cards/asafaya/bert-mini-arabic/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/aubmindlab/\n",
      "   creating: transformers-3.0.2/model_cards/aubmindlab/bert-base-arabert/\n",
      "  inflating: transformers-3.0.2/model_cards/aubmindlab/bert-base-arabert/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/aubmindlab/bert-base-arabertv01/\n",
      "  inflating: transformers-3.0.2/model_cards/aubmindlab/bert-base-arabertv01/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/bart-large-cnn/\n",
      " extracting: transformers-3.0.2/model_cards/bart-large-cnn/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/bart-large-xsum/\n",
      " extracting: transformers-3.0.2/model_cards/bart-large-xsum/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/bayartsogt/\n",
      "   creating: transformers-3.0.2/model_cards/bayartsogt/albert-mongolian/\n",
      "  inflating: transformers-3.0.2/model_cards/bayartsogt/albert-mongolian/README.md  \n",
      "  inflating: transformers-3.0.2/model_cards/bert-base-cased-README.md  \n",
      " extracting: transformers-3.0.2/model_cards/bert-base-chinese-README.md  \n",
      "  inflating: transformers-3.0.2/model_cards/bert-base-german-cased-README.md  \n",
      "  inflating: transformers-3.0.2/model_cards/bert-base-german-dbmdz-cased-README.md  \n",
      "  inflating: transformers-3.0.2/model_cards/bert-base-german-dbmdz-uncased-README.md  \n",
      "  inflating: transformers-3.0.2/model_cards/bert-base-multilingual-cased-README.md  \n",
      "  inflating: transformers-3.0.2/model_cards/bert-base-multilingual-uncased-README.md  \n",
      "  inflating: transformers-3.0.2/model_cards/bert-base-uncased-README.md  \n",
      " extracting: transformers-3.0.2/model_cards/bert-large-cased-README.md  \n",
      "   creating: transformers-3.0.2/model_cards/binwang/\n",
      "   creating: transformers-3.0.2/model_cards/binwang/xlnet-base-cased/\n",
      "  inflating: transformers-3.0.2/model_cards/binwang/xlnet-base-cased/README.md  \n",
      "  inflating: transformers-3.0.2/model_cards/camembert-base-README.md  \n",
      "   creating: transformers-3.0.2/model_cards/camembert/\n",
      "   creating: transformers-3.0.2/model_cards/camembert/camembert-base-ccnet-4gb/\n",
      "  inflating: transformers-3.0.2/model_cards/camembert/camembert-base-ccnet-4gb/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/camembert/camembert-base-ccnet/\n",
      "  inflating: transformers-3.0.2/model_cards/camembert/camembert-base-ccnet/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/camembert/camembert-base-oscar-4gb/\n",
      "  inflating: transformers-3.0.2/model_cards/camembert/camembert-base-oscar-4gb/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/camembert/camembert-base-wikipedia-4gb/\n",
      "  inflating: transformers-3.0.2/model_cards/camembert/camembert-base-wikipedia-4gb/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/camembert/camembert-large/\n",
      "  inflating: transformers-3.0.2/model_cards/camembert/camembert-large/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/canwenxu/\n",
      "   creating: transformers-3.0.2/model_cards/canwenxu/BERT-of-Theseus-MNLI/\n",
      "  inflating: transformers-3.0.2/model_cards/canwenxu/BERT-of-Theseus-MNLI/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/chrisliu298/\n",
      "   creating: transformers-3.0.2/model_cards/chrisliu298/arxiv_ai_gpt2/\n",
      "  inflating: transformers-3.0.2/model_cards/chrisliu298/arxiv_ai_gpt2/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/clue/\n",
      "   creating: transformers-3.0.2/model_cards/clue/albert_chinese_small/\n",
      "  inflating: transformers-3.0.2/model_cards/clue/albert_chinese_small/README.md  \n",
      "   creating: transformers-3.0.2/model_cards/clue/albert_chinese_tiny/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: transformers-3.0.2/model_cards/clue/albert_chinese_tiny/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/clue/roberta_chinese_3L312_clue_tiny/\r\n",
      "  inflating: transformers-3.0.2/model_cards/clue/roberta_chinese_3L312_clue_tiny/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/clue/roberta_chinese_base/\r\n",
      "  inflating: transformers-3.0.2/model_cards/clue/roberta_chinese_base/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/clue/roberta_chinese_large/\r\n",
      "  inflating: transformers-3.0.2/model_cards/clue/roberta_chinese_large/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/clue/xlnet_chinese_large/\r\n",
      "  inflating: transformers-3.0.2/model_cards/clue/xlnet_chinese_large/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/codegram/\r\n",
      "   creating: transformers-3.0.2/model_cards/codegram/calbert-base-uncased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/codegram/calbert-base-uncased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/daigo/\r\n",
      "   creating: transformers-3.0.2/model_cards/daigo/bert-base-japanese-sentiment/\r\n",
      "  inflating: transformers-3.0.2/model_cards/daigo/bert-base-japanese-sentiment/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/\r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-german-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-german-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-german-europeana-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-german-europeana-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-german-europeana-uncased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-german-europeana-uncased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-german-uncased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-german-uncased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-italian-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-italian-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-italian-uncased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-italian-uncased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-italian-xxl-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-italian-xxl-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-italian-xxl-uncased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-italian-xxl-uncased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-turkish-128k-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-turkish-128k-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-turkish-128k-uncased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-turkish-128k-uncased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-turkish-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-turkish-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/bert-base-turkish-uncased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/bert-base-turkish-uncased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/distilbert-base-turkish-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/distilbert-base-turkish-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/electra-base-turkish-cased-discriminator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/electra-base-turkish-cased-discriminator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dbmdz/electra-small-turkish-cased-discriminator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dbmdz/electra-small-turkish-cased-discriminator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/deepset/\r\n",
      "   creating: transformers-3.0.2/model_cards/deepset/bert-base-german-cased-oldvocab/\r\n",
      "  inflating: transformers-3.0.2/model_cards/deepset/bert-base-german-cased-oldvocab/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/deepset/quora_dedup_bert_base/\r\n",
      "  inflating: transformers-3.0.2/model_cards/deepset/quora_dedup_bert_base/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/deepset/roberta-base-squad2-covid/\r\n",
      "  inflating: transformers-3.0.2/model_cards/deepset/roberta-base-squad2-covid/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/deepset/roberta-base-squad2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/deepset/roberta-base-squad2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/deepset/sentence_bert/\r\n",
      "  inflating: transformers-3.0.2/model_cards/deepset/sentence_bert/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/digitalepidemiologylab/\r\n",
      "   creating: transformers-3.0.2/model_cards/digitalepidemiologylab/covid-twitter-bert/\r\n",
      "  inflating: transformers-3.0.2/model_cards/digitalepidemiologylab/covid-twitter-bert/README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/distilbert-base-multilingual-cased-README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/distilbert-base-uncased-README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/distilbert-base-uncased-distilled-squad-README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/distilgpt2-README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/distilroberta-base-README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/djstrong/\r\n",
      "   creating: transformers-3.0.2/model_cards/djstrong/bg_cs_pl_ru_cased_L-12_H-768_A-12/\r\n",
      "  inflating: transformers-3.0.2/model_cards/djstrong/bg_cs_pl_ru_cased_L-12_H-768_A-12/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dkleczek/\r\n",
      "   creating: transformers-3.0.2/model_cards/dkleczek/bert-base-polish-cased-v1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dkleczek/bert-base-polish-cased-v1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dkleczek/bert-base-polish-uncased-v1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dkleczek/bert-base-polish-uncased-v1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dumitrescustefan/\r\n",
      "   creating: transformers-3.0.2/model_cards/dumitrescustefan/bert-base-romanian-cased-v1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dumitrescustefan/bert-base-romanian-cased-v1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/dumitrescustefan/bert-base-romanian-uncased-v1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/dumitrescustefan/bert-base-romanian-uncased-v1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/elgeish/\r\n",
      "   creating: transformers-3.0.2/model_cards/elgeish/cs224n-squad2.0-albert-base-v2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/elgeish/cs224n-squad2.0-albert-base-v2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/elgeish/cs224n-squad2.0-albert-large-v2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/elgeish/cs224n-squad2.0-albert-large-v2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/elgeish/cs224n-squad2.0-albert-xxlarge-v1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/elgeish/cs224n-squad2.0-albert-xxlarge-v1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/elgeish/cs224n-squad2.0-distilbert-base-uncased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/elgeish/cs224n-squad2.0-distilbert-base-uncased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/elgeish/cs224n-squad2.0-roberta-base/\r\n",
      "  inflating: transformers-3.0.2/model_cards/elgeish/cs224n-squad2.0-roberta-base/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/emilyalsentzer/\r\n",
      "   creating: transformers-3.0.2/model_cards/emilyalsentzer/Bio_ClinicalBERT/\r\n",
      "  inflating: transformers-3.0.2/model_cards/emilyalsentzer/Bio_ClinicalBERT/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/emilyalsentzer/Bio_Discharge_Summary_BERT/\r\n",
      "  inflating: transformers-3.0.2/model_cards/emilyalsentzer/Bio_Discharge_Summary_BERT/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/facebook/\r\n",
      "   creating: transformers-3.0.2/model_cards/facebook/bart-large-cnn/\r\n",
      " extracting: transformers-3.0.2/model_cards/facebook/bart-large-cnn/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/facebook/bart-large/\r\n",
      "  inflating: transformers-3.0.2/model_cards/facebook/bart-large/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/fmikaelian/\r\n",
      "   creating: transformers-3.0.2/model_cards/fmikaelian/camembert-base-fquad/\r\n",
      "  inflating: transformers-3.0.2/model_cards/fmikaelian/camembert-base-fquad/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/fmikaelian/camembert-base-squad/\r\n",
      "  inflating: transformers-3.0.2/model_cards/fmikaelian/camembert-base-squad/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/fmikaelian/flaubert-base-uncased-squad/\r\n",
      "  inflating: transformers-3.0.2/model_cards/fmikaelian/flaubert-base-uncased-squad/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/fran-martinez/\r\n",
      "   creating: transformers-3.0.2/model_cards/fran-martinez/scibert_scivocab_cased_ner_jnlpba/\r\n",
      "  inflating: transformers-3.0.2/model_cards/fran-martinez/scibert_scivocab_cased_ner_jnlpba/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/gaochangkuan/\r\n",
      "   creating: transformers-3.0.2/model_cards/gaochangkuan/model_dir/\r\n",
      "  inflating: transformers-3.0.2/model_cards/gaochangkuan/model_dir/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/giganticode/\r\n",
      "   creating: transformers-3.0.2/model_cards/giganticode/StackOBERTflow-comments-small-v1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/giganticode/StackOBERTflow-comments-small-v1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/google/\r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-128_A-2/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-128_A-2/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-256_A-4/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-256_A-4/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-512_A-8/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-512_A-8/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-768_A-12/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-768_A-12/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-128_A-2/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-128_A-2/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-256_A-4/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-256_A-4/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-512_A-8/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-512_A-8/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-768_A-12/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-768_A-12/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-128_A-2/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-128_A-2/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-256_A-4/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-256_A-4/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-512_A-8/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-512_A-8/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-768_A-12/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-768_A-12/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-128_A-2/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-128_A-2/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-256_A-4/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-256_A-4/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-512_A-8/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-512_A-8/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-768_A-12/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-768_A-12/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-128_A-2/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-128_A-2/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-256_A-4/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-256_A-4/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-512_A-8/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-512_A-8/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-768_A-12/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-768_A-12/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-128_A-2/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-128_A-2/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-256_A-4/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-256_A-4/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-512_A-8/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-512_A-8/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-768_A-12/\r\n",
      "    linking: transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-768_A-12/README.md  -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/google/electra-base-discriminator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/google/electra-base-discriminator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/google/electra-base-generator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/google/electra-base-generator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/google/electra-large-discriminator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/google/electra-large-discriminator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/google/electra-large-generator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/google/electra-large-generator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/google/electra-small-discriminator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/google/electra-small-discriminator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/google/electra-small-generator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/google/electra-small-generator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/google/mobilebert-uncased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/google/mobilebert-uncased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/google/reformer-crime-and-punishment/\r\n",
      "  inflating: transformers-3.0.2/model_cards/google/reformer-crime-and-punishment/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/google/reformer-enwik8/\r\n",
      "  inflating: transformers-3.0.2/model_cards/google/reformer-enwik8/README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/gpt2-README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/gsarti/\r\n",
      "   creating: transformers-3.0.2/model_cards/gsarti/biobert-nli/\r\n",
      "  inflating: transformers-3.0.2/model_cards/gsarti/biobert-nli/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/gsarti/covidbert-nli/\r\n",
      "  inflating: transformers-3.0.2/model_cards/gsarti/covidbert-nli/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/gsarti/scibert-nli/\r\n",
      "  inflating: transformers-3.0.2/model_cards/gsarti/scibert-nli/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/healx/\r\n",
      "   creating: transformers-3.0.2/model_cards/healx/gpt-2-pubmed-large/\r\n",
      "  inflating: transformers-3.0.2/model_cards/healx/gpt-2-pubmed-large/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/healx/gpt-2-pubmed-medium/\r\n",
      "  inflating: transformers-3.0.2/model_cards/healx/gpt-2-pubmed-medium/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/henryk/\r\n",
      "   creating: transformers-3.0.2/model_cards/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huggingface/\r\n",
      "   creating: transformers-3.0.2/model_cards/huggingface/CodeBERTa-language-id/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huggingface/CodeBERTa-language-id/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huggingface/CodeBERTa-small-v1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huggingface/CodeBERTa-small-v1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/\r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/albert-base-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/albert-base-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/albert-tiny-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/albert-tiny-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/bert-base-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/bert-base-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/electra-base-discriminator-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/electra-base-discriminator-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/electra-base-generator-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/electra-base-generator-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/electra-small-discriminator-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/electra-small-discriminator-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/electra-small-generator-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/electra-small-generator-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/gpt2-117M-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/gpt2-117M-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/gpt2-345M-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/gpt2-345M-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/t5-base-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/t5-base-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/t5-small-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/t5-small-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/tiny-bert-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/tiny-bert-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/huseinzol05/xlnet-base-bahasa-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/huseinzol05/xlnet-base-bahasa-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/illuin/\r\n",
      "   creating: transformers-3.0.2/model_cards/illuin/camembert-base-fquad/\r\n",
      "  inflating: transformers-3.0.2/model_cards/illuin/camembert-base-fquad/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/illuin/camembert-large-fquad/\r\n",
      "  inflating: transformers-3.0.2/model_cards/illuin/camembert-large-fquad/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/ipuneetrathore/\r\n",
      "   creating: transformers-3.0.2/model_cards/ipuneetrathore/bert-base-cased-finetuned-finBERT/\r\n",
      "  inflating: transformers-3.0.2/model_cards/ipuneetrathore/bert-base-cased-finetuned-finBERT/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/iuliaturc/\r\n",
      "   creating: transformers-3.0.2/model_cards/iuliaturc/bert_uncased_L-2_H-128_A-2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/iuliaturc/bert_uncased_L-2_H-128_A-2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/ixa-ehu/\r\n",
      "   creating: transformers-3.0.2/model_cards/ixa-ehu/berteus-base-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/ixa-ehu/berteus-base-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/jannesg/\r\n",
      "   creating: transformers-3.0.2/model_cards/jannesg/bertsson/\r\n",
      "  inflating: transformers-3.0.2/model_cards/jannesg/bertsson/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/jplu/\r\n",
      "   creating: transformers-3.0.2/model_cards/jplu/tf-camembert-base/\r\n",
      "  inflating: transformers-3.0.2/model_cards/jplu/tf-camembert-base/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/jplu/tf-xlm-r-ner-40-lang/\r\n",
      "  inflating: transformers-3.0.2/model_cards/jplu/tf-xlm-r-ner-40-lang/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/jplu/tf-xlm-roberta-base/\r\n",
      "  inflating: transformers-3.0.2/model_cards/jplu/tf-xlm-roberta-base/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/jplu/tf-xlm-roberta-large/\r\n",
      "  inflating: transformers-3.0.2/model_cards/jplu/tf-xlm-roberta-large/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/julien-c/\r\n",
      "   creating: transformers-3.0.2/model_cards/julien-c/EsperBERTo-small-pos/\r\n",
      "  inflating: transformers-3.0.2/model_cards/julien-c/EsperBERTo-small-pos/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/julien-c/EsperBERTo-small/\r\n",
      "  inflating: transformers-3.0.2/model_cards/julien-c/EsperBERTo-small/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/julien-c/bert-xsmall-dummy/\r\n",
      "  inflating: transformers-3.0.2/model_cards/julien-c/bert-xsmall-dummy/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/julien-c/dummy-unknown/\r\n",
      "  inflating: transformers-3.0.2/model_cards/julien-c/dummy-unknown/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/krevas/\r\n",
      "   creating: transformers-3.0.2/model_cards/krevas/finance-koelectra-base-discriminator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/krevas/finance-koelectra-base-discriminator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/krevas/finance-koelectra-base-generator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/krevas/finance-koelectra-base-generator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/krevas/finance-koelectra-small-discriminator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/krevas/finance-koelectra-small-discriminator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/krevas/finance-koelectra-small-generator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/krevas/finance-koelectra-small-generator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/ktrapeznikov/\r\n",
      "   creating: transformers-3.0.2/model_cards/ktrapeznikov/albert-xlarge-v2-squad-v2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/ktrapeznikov/albert-xlarge-v2-squad-v2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/ktrapeznikov/biobert_v1.1_pubmed_squad_v2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/ktrapeznikov/biobert_v1.1_pubmed_squad_v2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/ktrapeznikov/scibert_scivocab_uncased_squad_v2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/ktrapeznikov/scibert_scivocab_uncased_squad_v2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/lserinol/\r\n",
      "   creating: transformers-3.0.2/model_cards/lserinol/bert-turkish-question-answering/\r\n",
      "  inflating: transformers-3.0.2/model_cards/lserinol/bert-turkish-question-answering/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/lvwerra/\r\n",
      "   creating: transformers-3.0.2/model_cards/lvwerra/bert-imdb/\r\n",
      "  inflating: transformers-3.0.2/model_cards/lvwerra/bert-imdb/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/lvwerra/gpt2-imdb-ctrl/\r\n",
      "  inflating: transformers-3.0.2/model_cards/lvwerra/gpt2-imdb-ctrl/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/lvwerra/gpt2-imdb-pos/\r\n",
      "  inflating: transformers-3.0.2/model_cards/lvwerra/gpt2-imdb-pos/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/lvwerra/gpt2-imdb/\r\n",
      "  inflating: transformers-3.0.2/model_cards/lvwerra/gpt2-imdb/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/lvwerra/gpt2-medium-taboo/\r\n",
      "  inflating: transformers-3.0.2/model_cards/lvwerra/gpt2-medium-taboo/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/lysandre/\r\n",
      "   creating: transformers-3.0.2/model_cards/lysandre/arxiv-nlp/\r\n",
      "  inflating: transformers-3.0.2/model_cards/lysandre/arxiv-nlp/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/lysandre/arxiv/\r\n",
      "  inflating: transformers-3.0.2/model_cards/lysandre/arxiv/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/microsoft/\r\n",
      "   creating: transformers-3.0.2/model_cards/microsoft/DialoGPT-large/\r\n",
      "  inflating: transformers-3.0.2/model_cards/microsoft/DialoGPT-large/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/microsoft/DialoGPT-medium/\r\n",
      "  inflating: transformers-3.0.2/model_cards/microsoft/DialoGPT-medium/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/microsoft/DialoGPT-small/\r\n",
      "  inflating: transformers-3.0.2/model_cards/microsoft/DialoGPT-small/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/microsoft/MiniLM-L12-H384-uncased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/microsoft/MiniLM-L12-H384-uncased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/microsoft/Multilingual-MiniLM-L12-H384/\r\n",
      "  inflating: transformers-3.0.2/model_cards/microsoft/Multilingual-MiniLM-L12-H384/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/monologg/\r\n",
      "   creating: transformers-3.0.2/model_cards/monologg/koelectra-base-discriminator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/monologg/koelectra-base-discriminator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/monologg/koelectra-base-generator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/monologg/koelectra-base-generator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/monologg/koelectra-small-discriminator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/monologg/koelectra-small-discriminator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/monologg/koelectra-small-generator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/monologg/koelectra-small-generator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/monsoon-nlp/\r\n",
      "   creating: transformers-3.0.2/model_cards/monsoon-nlp/hindi-bert/\r\n",
      "  inflating: transformers-3.0.2/model_cards/monsoon-nlp/hindi-bert/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/moumeneb1/\r\n",
      "   creating: transformers-3.0.2/model_cards/moumeneb1/flaubert-base-cased-ecology_crisis/\r\n",
      "  inflating: transformers-3.0.2/model_cards/moumeneb1/flaubert-base-cased-ecology_crisis/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/\r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/CodeBERTaPy/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/CodeBERTaPy/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/GPT-2-finetuned-CORD19/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/GPT-2-finetuned-CORD19/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/GPT-2-finetuned-covid-bio-medrxiv/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/GPT-2-finetuned-covid-bio-medrxiv/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/RuPERTa-base-finetuned-ner/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/RuPERTa-base-finetuned-ner/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/RuPERTa-base-finetuned-pos/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/RuPERTa-base-finetuned-pos/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/TinyBERT-spanish-uncased-finetuned-ner/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/TinyBERT-spanish-uncased-finetuned-ner/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-italian-finedtuned-squadv1-it-alfa/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-italian-finedtuned-squadv1-it-alfa/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-medium-finetuned-squadv2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-medium-finetuned-squadv2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-mini-finetuned-squadv2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-mini-finetuned-squadv2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-multi-cased-finedtuned-xquad-tydiqa-goldp/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-multi-cased-finedtuned-xquad-tydiqa-goldp/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-multi-cased-finetuned-xquadv1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-multi-cased-finetuned-xquadv1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-multi-uncased-finetuned-xquadv1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-multi-uncased-finetuned-xquadv1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-small-finetuned-squadv2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-small-finetuned-squadv2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-small-finetuned-typo-detection/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-small-finetuned-typo-detection/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-spanish-cased-finetuned-ner/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-spanish-cased-finetuned-ner/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-spanish-cased-finetuned-pos-syntax/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-spanish-cased-finetuned-pos-syntax/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-spanish-cased-finetuned-pos/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-spanish-cased-finetuned-pos/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-tiny-finetuned-squadv2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-tiny-finetuned-squadv2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/bert-uncased-finetuned-qnli/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/bert-uncased-finetuned-qnli/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/chEMBL_smiles_v1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/chEMBL_smiles_v1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/codeBERTaJS/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/codeBERTaJS/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/distilbert-base-multi-cased-finetuned-typo-detection/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/distilbert-base-multi-cased-finetuned-typo-detection/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/distilbert-multi-finetuned-for-xqua-on-tydiqa/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/distilbert-multi-finetuned-for-xqua-on-tydiqa/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/distilroberta-base-finetuned-sentiment/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/distilroberta-base-finetuned-sentiment/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/electra-base-finetuned-squadv1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/electra-base-finetuned-squadv1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/electra-small-finetuned-squadv2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/electra-small-finetuned-squadv2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/electricidad-small-discriminator/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/electricidad-small-discriminator/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/electricidad-small-finetuned-squadv1-es/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/electricidad-small-finetuned-squadv1-es/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/gpt2-imdb-neg/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/gpt2-imdb-neg/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/gpt2-imdb-neutral/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/gpt2-imdb-neutral/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/longformer-base-4096-finetuned-squadv2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/longformer-base-4096-finetuned-squadv2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/roberta-large-finetuned-wsc/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/roberta-large-finetuned-wsc/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/spanbert-base-finetuned-squadv1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/spanbert-base-finetuned-squadv1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/spanbert-base-finetuned-squadv2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/spanbert-base-finetuned-squadv2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/spanbert-base-finetuned-tacred/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/spanbert-base-finetuned-tacred/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/spanbert-finetuned-squadv1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/spanbert-finetuned-squadv1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/spanbert-finetuned-squadv2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/spanbert-finetuned-squadv2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/spanbert-large-finetuned-squadv1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/spanbert-large-finetuned-squadv1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/spanbert-large-finetuned-squadv2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/spanbert-large-finetuned-squadv2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/spanbert-large-finetuned-tacred/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/spanbert-large-finetuned-tacred/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-emotion/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-emotion/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-imdb-sentiment/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-imdb-sentiment/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-sarcasm-twitter/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-sarcasm-twitter/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-span-sentiment-extraction/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-span-sentiment-extraction/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-squadv2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-squadv2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-summarize-news/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/t5-base-finetuned-summarize-news/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/mrm8488/xlm-multi-finetuned-xquadv1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/mrm8488/xlm-multi-finetuned-xquadv1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/nlpaueb/\r\n",
      "   creating: transformers-3.0.2/model_cards/nlpaueb/bert-base-greek-uncased-v1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/nlpaueb/bert-base-greek-uncased-v1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/nlptown/\r\n",
      "   creating: transformers-3.0.2/model_cards/nlptown/bert-base-multilingual-uncased-sentiment/\r\n",
      "  inflating: transformers-3.0.2/model_cards/nlptown/bert-base-multilingual-uncased-sentiment/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/\r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-base-100M-1/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-base-100M-1/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-base-100M-2/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-base-100M-2/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-base-100M-3/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-base-100M-3/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-base-10M-1/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-base-10M-1/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-base-10M-2/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-base-10M-2/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-base-10M-3/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-base-10M-3/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-base-1B-1/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-base-1B-1/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-base-1B-2/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-base-1B-2/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-base-1B-3/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-base-1B-3/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-med-small-1M-1/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-med-small-1M-1/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-med-small-1M-2/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-med-small-1M-2/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta-med-small-1M-3/\r\n",
      "    linking: transformers-3.0.2/model_cards/nyu-mll/roberta-med-small-1M-3/README.md  -> ../roberta_1M_to_1B/README.md \r\n",
      "   creating: transformers-3.0.2/model_cards/nyu-mll/roberta_1M_to_1B/\r\n",
      "  inflating: transformers-3.0.2/model_cards/nyu-mll/roberta_1M_to_1B/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/oliverguhr/\r\n",
      "   creating: transformers-3.0.2/model_cards/oliverguhr/german-sentiment-bert/\r\n",
      "  inflating: transformers-3.0.2/model_cards/oliverguhr/german-sentiment-bert/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/pradhyra/\r\n",
      "   creating: transformers-3.0.2/model_cards/pradhyra/AWSBlogBert/\r\n",
      "  inflating: transformers-3.0.2/model_cards/pradhyra/AWSBlogBert/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/redewiedergabe/\r\n",
      "   creating: transformers-3.0.2/model_cards/redewiedergabe/bert-base-historical-german-rw-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/redewiedergabe/bert-base-historical-german-rw-cased/README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/roberta-base-README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/roberta-large-README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/roberta-large-mnli-README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/savasy/\r\n",
      "   creating: transformers-3.0.2/model_cards/savasy/bert-base-turkish-ner-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/savasy/bert-base-turkish-ner-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/savasy/bert-base-turkish-sentiment-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/savasy/bert-base-turkish-sentiment-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/savasy/bert-base-turkish-squad/\r\n",
      "  inflating: transformers-3.0.2/model_cards/savasy/bert-base-turkish-squad/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/schmidek/\r\n",
      "   creating: transformers-3.0.2/model_cards/schmidek/electra-small-cased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/schmidek/electra-small-cased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/seiya/\r\n",
      "   creating: transformers-3.0.2/model_cards/seiya/oubiobert-base-uncased/\r\n",
      "  inflating: transformers-3.0.2/model_cards/seiya/oubiobert-base-uncased/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/severinsimmler/\r\n",
      "   creating: transformers-3.0.2/model_cards/severinsimmler/literary-german-bert/\r\n",
      "  inflating: transformers-3.0.2/model_cards/severinsimmler/literary-german-bert/README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/severinsimmler/literary-german-bert/kfold.png  \r\n",
      "  inflating: transformers-3.0.2/model_cards/severinsimmler/literary-german-bert/prosa-jahre.png  \r\n",
      "   creating: transformers-3.0.2/model_cards/seyonec/\r\n",
      "   creating: transformers-3.0.2/model_cards/seyonec/ChemBERTa-zinc-base-v1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/seyonec/ChemBERTa-zinc-base-v1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/shoarora/\r\n",
      "   creating: transformers-3.0.2/model_cards/shoarora/alectra-small-owt/\r\n",
      "  inflating: transformers-3.0.2/model_cards/shoarora/alectra-small-owt/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/shoarora/electra-small-owt/\r\n",
      "  inflating: transformers-3.0.2/model_cards/shoarora/electra-small-owt/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/spentaur/\r\n",
      "   creating: transformers-3.0.2/model_cards/spentaur/yelp/\r\n",
      "  inflating: transformers-3.0.2/model_cards/spentaur/yelp/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/surajp/\r\n",
      "   creating: transformers-3.0.2/model_cards/surajp/SanBERTa/\r\n",
      "  inflating: transformers-3.0.2/model_cards/surajp/SanBERTa/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/surajp/albert-base-sanskrit/\r\n",
      "  inflating: transformers-3.0.2/model_cards/surajp/albert-base-sanskrit/README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/t5-11b-README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/t5-3b-README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/t5-base-README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/t5-large-README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/t5-small-README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/tblard/\r\n",
      "   creating: transformers-3.0.2/model_cards/tblard/tf-allocine/\r\n",
      "  inflating: transformers-3.0.2/model_cards/tblard/tf-allocine/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/twmkn9/\r\n",
      "   creating: transformers-3.0.2/model_cards/twmkn9/albert-base-v2-squad2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/twmkn9/albert-base-v2-squad2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/twmkn9/bert-base-uncased-squad2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/twmkn9/bert-base-uncased-squad2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/twmkn9/distilbert-base-uncased-squad2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/twmkn9/distilbert-base-uncased-squad2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/twmkn9/distilroberta-base-squad2/\r\n",
      "  inflating: transformers-3.0.2/model_cards/twmkn9/distilroberta-base-squad2/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/valhalla/\r\n",
      "   creating: transformers-3.0.2/model_cards/valhalla/bart-large-finetuned-squadv1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/valhalla/bart-large-finetuned-squadv1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/valhalla/electra-base-discriminator-finetuned_squadv1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/valhalla/electra-base-discriminator-finetuned_squadv1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/valhalla/longformer-base-4096-finetuned-squadv1/\r\n",
      "  inflating: transformers-3.0.2/model_cards/valhalla/longformer-base-4096-finetuned-squadv1/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/valhalla/t5-base-squad/\r\n",
      "  inflating: transformers-3.0.2/model_cards/valhalla/t5-base-squad/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/voidful/\r\n",
      "   creating: transformers-3.0.2/model_cards/voidful/albert_chinese_base/\r\n",
      "  inflating: transformers-3.0.2/model_cards/voidful/albert_chinese_base/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/voidful/albert_chinese_large/\r\n",
      "  inflating: transformers-3.0.2/model_cards/voidful/albert_chinese_large/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/voidful/albert_chinese_small/\r\n",
      "  inflating: transformers-3.0.2/model_cards/voidful/albert_chinese_small/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/voidful/albert_chinese_tiny/\r\n",
      "  inflating: transformers-3.0.2/model_cards/voidful/albert_chinese_tiny/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/voidful/albert_chinese_xlarge/\r\n",
      "  inflating: transformers-3.0.2/model_cards/voidful/albert_chinese_xlarge/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/voidful/albert_chinese_xxlarge/\r\n",
      "  inflating: transformers-3.0.2/model_cards/voidful/albert_chinese_xxlarge/README.md  \r\n",
      "   creating: transformers-3.0.2/model_cards/wptoux/\r\n",
      "   creating: transformers-3.0.2/model_cards/wptoux/albert-chinese-large-qa/\r\n",
      "  inflating: transformers-3.0.2/model_cards/wptoux/albert-chinese-large-qa/README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/xlm-mlm-en-2048-README.md  \r\n",
      "  inflating: transformers-3.0.2/model_cards/xlm-roberta-base-README.md  \r\n",
      "   creating: transformers-3.0.2/notebooks/\r\n",
      "  inflating: transformers-3.0.2/notebooks/01-training-tokenizers.ipynb  \r\n",
      "  inflating: transformers-3.0.2/notebooks/02-transformers.ipynb  \r\n",
      "  inflating: transformers-3.0.2/notebooks/03-pipelines.ipynb  \r\n",
      "  inflating: transformers-3.0.2/notebooks/04-onnx-export.ipynb  \r\n",
      "  inflating: transformers-3.0.2/notebooks/05-benchmark.ipynb  \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: transformers-3.0.2/notebooks/README.md  \r\n",
      "  inflating: transformers-3.0.2/setup.cfg  \r\n",
      "  inflating: transformers-3.0.2/setup.py  \r\n",
      "   creating: transformers-3.0.2/src/\r\n",
      "   creating: transformers-3.0.2/src/transformers/\r\n",
      "  inflating: transformers-3.0.2/src/transformers/__init__.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/activations.py  \r\n",
      "   creating: transformers-3.0.2/src/transformers/benchmark/\r\n",
      " extracting: transformers-3.0.2/src/transformers/benchmark/__init__.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/benchmark/benchmark.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/benchmark/benchmark_args.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/benchmark/benchmark_args_tf.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/benchmark/benchmark_args_utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/benchmark/benchmark_tf.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/benchmark/benchmark_utils.py  \r\n",
      "   creating: transformers-3.0.2/src/transformers/commands/\r\n",
      "  inflating: transformers-3.0.2/src/transformers/commands/__init__.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/commands/convert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/commands/download.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/commands/env.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/commands/run.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/commands/serving.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/commands/train.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/commands/transformers_cli.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/commands/user.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_albert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_auto.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_bart.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_bert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_camembert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_ctrl.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_distilbert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_electra.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_encoder_decoder.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_flaubert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_gpt2.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_longformer.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_marian.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_mmbt.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_mobilebert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_openai.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_reformer.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_retribert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_t5.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_transfo_xl.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_xlm.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_xlm_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/configuration_xlnet.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_graph_to_onnx.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_longformer_original_pytorch_lightning_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_marian_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_mobilebert_original_tf_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_pytorch_checkpoint_to_tf2.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py  \r\n",
      "   creating: transformers-3.0.2/src/transformers/data/\r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/__init__.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/data_collator.py  \r\n",
      "   creating: transformers-3.0.2/src/transformers/data/datasets/\r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/datasets/__init__.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/datasets/glue.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/datasets/language_modeling.py  \r\n",
      "   creating: transformers-3.0.2/src/transformers/data/metrics/\r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/metrics/__init__.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/metrics/squad_metrics.py  \r\n",
      "   creating: transformers-3.0.2/src/transformers/data/processors/\r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/processors/__init__.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/processors/glue.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/processors/squad.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/processors/utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/data/processors/xnli.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/file_utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/generation_tf_utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/generation_utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/hf_api.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/hf_argparser.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modelcard.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_albert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_auto.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_bart.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_bert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_camembert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_ctrl.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_distilbert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_electra.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_encoder_decoder.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_flaubert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_gpt2.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_longformer.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_marian.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_mmbt.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_mobilebert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_openai.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_reformer.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_retribert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_t5.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_albert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_auto.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_bert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_camembert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_ctrl.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_distilbert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_electra.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_flaubert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_gpt2.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_mobilebert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_openai.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_pytorch_utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_t5.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_transfo_xl.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_transfo_xl_utilities.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_xlm.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_xlm_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_tf_xlnet.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_transfo_xl.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_transfo_xl_utilities.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_xlm.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_xlm_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/modeling_xlnet.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/optimization.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/optimization_tf.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/pipelines.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/testing_utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_albert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_auto.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_bart.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_bert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_bert_japanese.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_camembert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_ctrl.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_distilbert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_electra.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_flaubert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_gpt2.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_longformer.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_marian.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_mobilebert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_openai.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_reformer.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_retribert.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_t5.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_transfo_xl.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_utils_base.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_utils_fast.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_xlm.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_xlm_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/tokenization_xlnet.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/trainer.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/trainer_tf.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/trainer_utils.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/training_args.py  \r\n",
      "  inflating: transformers-3.0.2/src/transformers/training_args_tf.py  \r\n",
      "   creating: transformers-3.0.2/templates/\r\n",
      "   creating: transformers-3.0.2/templates/adding_a_new_example_script/\r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_example_script/README.md  \r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_example_script/run_xxx.py  \r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_example_script/utils_xxx.py  \r\n",
      "   creating: transformers-3.0.2/templates/adding_a_new_model/\r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_model/README.md  \r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_model/configuration_xxx.py  \r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py  \r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_model/modeling_tf_xxx.py  \r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_model/modeling_xxx.py  \r\n",
      "   creating: transformers-3.0.2/templates/adding_a_new_model/tests/\r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py  \r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_model/tests/test_modeling_xxx.py  \r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_model/tests/test_tokenization_xxx.py  \r\n",
      "  inflating: transformers-3.0.2/templates/adding_a_new_model/tokenization_xxx.py  \r\n",
      "   creating: transformers-3.0.2/tests/\r\n",
      " extracting: transformers-3.0.2/tests/__init__.py  \r\n",
      "   creating: transformers-3.0.2/tests/fixtures/\r\n",
      " extracting: transformers-3.0.2/tests/fixtures/dummy-config.json  \r\n",
      " extracting: transformers-3.0.2/tests/fixtures/empty.txt  \r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/input.txt  \r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/sample_text.txt  \r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/spiece.model  \r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/test_sentencepiece.model  \r\n",
      "   creating: transformers-3.0.2/tests/fixtures/tests_samples/\r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/tests_samples/.gitignore  \r\n",
      "   creating: transformers-3.0.2/tests/fixtures/tests_samples/GermEval/\r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/tests_samples/GermEval/dev.txt  \r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/tests_samples/GermEval/labels.txt  \r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/tests_samples/GermEval/train.txt  \r\n",
      "   creating: transformers-3.0.2/tests/fixtures/tests_samples/MRPC/\r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/tests_samples/MRPC/dev.tsv  \r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/tests_samples/MRPC/train.tsv  \r\n",
      "   creating: transformers-3.0.2/tests/fixtures/tests_samples/SQUAD/\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: transformers-3.0.2/tests/fixtures/tests_samples/SQUAD/dev-v2.0.json  \r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/tests_samples/SQUAD/train-v2.0.json  \r\n",
      "   creating: transformers-3.0.2/tests/fixtures/tests_samples/STS-B/\r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/tests_samples/STS-B/dev.tsv  \r\n",
      "  inflating: transformers-3.0.2/tests/fixtures/tests_samples/STS-B/train.tsv  \r\n",
      "  inflating: transformers-3.0.2/tests/test_activations.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_benchmark.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_benchmark_tf.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_configuration_auto.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_configuration_common.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_doc_samples.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_hf_api.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_hf_argparser.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_model_card.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_albert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_auto.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_bart.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_bert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_camembert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_common.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_ctrl.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_distilbert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_electra.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_encoder_decoder.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_flaubert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_gpt2.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_longformer.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_marian.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_mobilebert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_openai.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_reformer.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_t5.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_albert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_auto.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_bert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_camembert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_common.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_ctrl.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_distilbert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_electra.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_flaubert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_gpt2.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_mobilebert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_openai_gpt.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_t5.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_transfo_xl.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_xlm.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_xlm_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_tf_xlnet.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_transfo_xl.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_xlm.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_xlm_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_modeling_xlnet.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_onnx.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_optimization.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_optimization_tf.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_pipelines.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_albert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_auto.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_bert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_bert_japanese.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_common.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_ctrl.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_distilbert.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_fast.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_gpt2.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_marian.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_openai.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_t5.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_transfo_xl.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_utils.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_xlm.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_xlm_roberta.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_tokenization_xlnet.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_trainer.py  \r\n",
      "  inflating: transformers-3.0.2/tests/test_trainer_distributed.py  \r\n",
      "   creating: transformers-3.0.2/utils/\r\n",
      "  inflating: transformers-3.0.2/utils/download_glue_data.py  \r\n",
      "  inflating: transformers-3.0.2/utils/link_tester.py  \r\n",
      "  inflating: transformers-3.0.2/valohai.yaml  \r\n",
      "finishing deferred symbolic links:\r\n",
      "  transformers-3.0.2/docs/source/contributing.md -> ../../CONTRIBUTING.md\r\n",
      "  transformers-3.0.2/docs/source/examples.md -> ../../examples/README.md\r\n",
      "  transformers-3.0.2/docs/source/notebooks.md -> ../../notebooks/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-128_A-2/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-256_A-4/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-512_A-8/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-10_H-768_A-12/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-128_A-2/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-256_A-4/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-512_A-8/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-12_H-768_A-12/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-128_A-2/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-256_A-4/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-512_A-8/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-2_H-768_A-12/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-128_A-2/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-256_A-4/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-512_A-8/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-4_H-768_A-12/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-128_A-2/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-256_A-4/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-512_A-8/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-6_H-768_A-12/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-128_A-2/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-256_A-4/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-512_A-8/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/google/bert_uncased_L-8_H-768_A-12/README.md -> ../../iuliaturc/bert_uncased_L-2_H-128_A-2/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-base-100M-1/README.md -> ../roberta_1M_to_1B/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-base-100M-2/README.md -> ../roberta_1M_to_1B/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-base-100M-3/README.md -> ../roberta_1M_to_1B/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-base-10M-1/README.md -> ../roberta_1M_to_1B/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-base-10M-2/README.md -> ../roberta_1M_to_1B/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-base-10M-3/README.md -> ../roberta_1M_to_1B/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-base-1B-1/README.md -> ../roberta_1M_to_1B/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-base-1B-2/README.md -> ../roberta_1M_to_1B/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-base-1B-3/README.md -> ../roberta_1M_to_1B/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-med-small-1M-1/README.md -> ../roberta_1M_to_1B/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-med-small-1M-2/README.md -> ../roberta_1M_to_1B/README.md\r\n",
      "  transformers-3.0.2/model_cards/nyu-mll/roberta-med-small-1M-3/README.md -> ../roberta_1M_to_1B/README.md\r\n"
     ]
    }
   ],
   "source": [
    "!unzip transformers-3.0.2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cd070dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/workplace/GiTi-4/GiTi-4/transformers-3.0.2\n"
     ]
    }
   ],
   "source": [
    "%cd transformers-3.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c66973a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "codecov.yml\t\t     docs\t Makefile     README.md  templates\n",
      "CONTRIBUTING.md\t\t     examples\t MANIFEST.in  setup.cfg  tests\n",
      "deploy_multi_version_doc.sh  hubconf.py  model_cards  setup.py\t utils\n",
      "docker\t\t\t     LICENSE\t notebooks    src\t valohai.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dccf540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e7538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eed88f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (4.28.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.7.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2021.11.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.9/site-packages (12.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from pyarrow) (1.21.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting nlp\n",
      "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
      "     || 1.7 MB 6.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from nlp) (4.62.3)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /opt/conda/lib/python3.9/site-packages (from nlp) (12.0.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from nlp) (1.3.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from nlp) (1.21.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from nlp) (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from nlp) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from nlp) (3.12.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from nlp) (2.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->nlp) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->nlp) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->nlp) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->nlp) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->nlp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->nlp) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.16.0)\n",
      "Installing collected packages: nlp\n",
      "Successfully installed nlp-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting captum==0.2.0\n",
      "  Downloading captum-0.2.0-py3-none-any.whl (1.4 MB)\n",
      "     || 1.4 MB 6.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.2 in /opt/conda/lib/python3.9/site-packages (from captum==0.2.0) (1.12.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from captum==0.2.0) (3.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from captum==0.2.0) (1.21.4)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.2->captum==0.2.0) (4.7.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->captum==0.2.0) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib->captum==0.2.0) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->captum==0.2.0) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->captum==0.2.0) (8.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->captum==0.2.0) (3.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->captum==0.2.0) (1.16.0)\n",
      "Installing collected packages: captum\n",
      "Successfully installed captum-0.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (2.13.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "     || 521 kB 6.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2.26.0)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "     || 166 kB 76.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets) (1.3.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (0.3.4)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets) (0.70.12.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface-hub>=0.18.0\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "     || 311 kB 65.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.8)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.18.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.18.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: fsspec, pyarrow-hotfix, huggingface-hub, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.11.1\n",
      "    Uninstalling fsspec-2021.11.1:\n",
      "      Successfully uninstalled fsspec-2021.11.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.13.1\n",
      "    Uninstalling datasets-2.13.1:\n",
      "      Successfully uninstalled datasets-2.13.1\n",
      "Successfully installed datasets-2.15.0 fsspec-2023.10.0 huggingface-hub-0.19.4 pyarrow-hotfix-0.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pyarrow\n",
    "!pip install nlp\n",
    "!pip install captum==0.2.0\n",
    "!pip install datasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a70e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d40e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.28.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nlp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from captum.attr import (IntegratedGradients, LayerIntegratedGradients,\n",
    "                         configure_interpretable_embedding_layer,\n",
    "                         remove_interpretable_embedding_layer)\n",
    "from captum.attr import visualization as viz\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (ElectraForSequenceClassification,\n",
    "                          ElectraTokenizerFast, EvalPrediction, InputFeatures,\n",
    "                          Trainer, TrainingArguments, glue_compute_metrics)\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "541204ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf4df9361084b319a87fba213aa9720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3889999c5684432eaf33587bc97c355e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/54.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00398a0a05fa42ea929330113a305514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa27b92ac5964b8ead6d5d24160cc087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c7cbf86b8c4e0b9b30f5c994d8d0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ElectraForSequenceClassification.from_pretrained(\n",
    "    \"google/electra-small-discriminator\", num_labels = 2)\n",
    "\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\n",
    "    \"google/electra-small-discriminator\", do_lower_case=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a4ad58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdac3a99e9e449ceaef8860a3c72160e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/28.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d42a381035f48e1b17769e6ecca8c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376cc5c0cbd444cf9f5a2979bf001b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/27.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a007a7ea9f418a85399b2c7f0f0673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97606f66b913471ababf2248371e395c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bdc1b9dbde485481b31b6d54916e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b55d2c70c84b7bb902f945722b8a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set labels: {0, 1}\n",
      "Validation set labels: {0, 1}\n",
      "Test set labels: {-1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates something rather beautiful about human nature</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same throughout</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichs the filmmakers could dredge up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    sentence  \\\n",
       "0  hide new secretions from the parental units                                                 \n",
       "1  contains no wit , only labored gags                                                         \n",
       "2  that loves its characters and communicates something rather beautiful about human nature    \n",
       "3  remains utterly satisfied to remain the same throughout                                     \n",
       "4  on the worst revenge-of-the-nerds clichs the filmmakers could dredge up                    \n",
       "\n",
       "   label  \n",
       "0  0      \n",
       "1  0      \n",
       "2  1      \n",
       "3  0      \n",
       "4  0      "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the SST2 dataset from the nlp library\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"glue\",\"sst2\")\n",
    "\n",
    "# Look at the labels\n",
    "print(\"Training set labels: {}\".format(set(dataset[\"train\"][\"label\"])))\n",
    "print(\"Validation set labels: {}\".format(set(dataset[\"validation\"][\"label\"])))\n",
    "print(\"Test set labels: {}\".format(set(dataset[\"test\"][\"label\"])))\n",
    "\n",
    "# Explore the dataset\n",
    "df = pd.DataFrame({\"sentence\": dataset[\"train\"][\"sentence\"],\n",
    "                   \"label\": dataset[\"train\"][\"label\"]})\n",
    "pd.options.display.max_colwidth = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be1fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Tokenize the input\n",
    "        self.tokenized_inputs = tokenizer(inputs, padding=True)   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return InputFeatures(\n",
    "            input_ids=self.tokenized_inputs['input_ids'][idx],\n",
    "            token_type_ids=self.tokenized_inputs['token_type_ids'][idx],\n",
    "            attention_mask=self.tokenized_inputs['attention_mask'][idx],\n",
    "            label=self.targets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d939d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainerDataset(dataset[\"train\"][\"sentence\"],\n",
    "                               dataset[\"train\"][\"label\"], tokenizer)\n",
    "eval_dataset = TrainerDataset(dataset[\"validation\"][\"sentence\"],\n",
    "                              dataset[\"validation\"][\"label\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05627334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/model_electra\", # ./models/model_electra\n",
    "    num_train_epochs=3,  # 1 (1 epoch gives slightly lower accuracy)\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=32,\n",
    "#     evaluate_during_training=True,     \n",
    "    dataloader_drop_last=True,  # Make sure all batches are of equal size\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    # The choice of a dataset (task_name) implies metric\n",
    "    return glue_compute_metrics(\n",
    "        task_name=\"sst-2\",\n",
    "        preds=preds,\n",
    "        labels=p.label_ids)\n",
    "\n",
    "\n",
    "# Instantiate the Trainer class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb8638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6312' max='6312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6312/6312 07:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.184900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.159600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6312, training_loss=0.188323605952003, metrics={'train_runtime': 464.0276, 'train_samples_per_second': 435.42, 'train_steps_per_second': 13.603, 'total_flos': 765999188342784.0, 'train_loss': 0.188323605952003, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d275ecbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9231651376146789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the  Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the  Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_result = trainer.evaluate()\n",
    "print(\"Accuracy: {}\".format(model_result[\"eval_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b15021d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"visually imaginative , thematically instructive and thoroughly \\\n",
    "delightful , it takes us on a roller-coaster ride from innocence to experience \\\n",
    "without even a hint of that typical kiddie-flick sentimentality. \"\n",
    "true_label = 1\n",
    "\n",
    "[x for x in dataset[\"validation\"] if x[\"sentence\"] == text]\n",
    "\n",
    "\n",
    "# type(text)  # str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6fc55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669e6ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cpu')\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6886831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_interpretable_embeddings():\n",
    "    \"\"\"Configure interpretable embedding layer\"\"\"\n",
    "    interpretable_embedding1 = configure_interpretable_embedding_layer(\n",
    "        model, \"electra.embeddings.word_embeddings\")\n",
    "    interpretable_embedding2 = configure_interpretable_embedding_layer(\n",
    "        model, \"electra.embeddings.token_type_embeddings\")\n",
    "    interpretable_embedding3 = configure_interpretable_embedding_layer(\n",
    "        model,\"electra.embeddings.position_embeddings\")\n",
    "    return (interpretable_embedding1,\n",
    "            interpretable_embedding2,\n",
    "            interpretable_embedding3)\n",
    "\n",
    "\n",
    "def remove_interpretable_embeddings(interpretable_embedding1, \n",
    "                                    interpretable_embedding2, \n",
    "                                    interpretable_embedding3):\n",
    "    \"\"\"Remove interpretable layer to restore the original model structure\"\"\"\n",
    "    if not \\\n",
    "    type(model.get_input_embeddings()).__name__ == \"InterpretableEmbeddingBase\":\n",
    "        return\n",
    "    remove_interpretable_embedding_layer(model, interpretable_embedding1)\n",
    "    remove_interpretable_embedding_layer(model, interpretable_embedding2)\n",
    "    remove_interpretable_embedding_layer(model, interpretable_embedding3)  \n",
    "\n",
    "\n",
    "def predict_forward_func(input_ids, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    \"\"\"Function passed to ig constructors\"\"\"\n",
    "    return model(input_ids=input_ids, \n",
    "                 token_type_ids=token_type_ids, \n",
    "                 position_ids=position_ids, \n",
    "                 attention_mask=attention_mask)[0]  \n",
    "\n",
    "\n",
    "def prepare_input(text):\n",
    "    \"\"\"Prepare ig attribution input: tokenize sample and baseline text.\"\"\"\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\", \n",
    "                               return_attention_mask=True)\n",
    "    seq_len = tokenized_text[\"input_ids\"].shape[1]\n",
    "    position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    # Construct the baseline (a reference sample).\n",
    "    # A sequence of [PAD] tokens of length equal to that of the processed sample\n",
    "    ref_text = tokenizer.pad_token * (seq_len - 2) # special tokens\n",
    "    tokenized_ref_text = tokenizer(ref_text, return_tensors=\"pt\") \n",
    "    ref_position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    return (tokenized_text[\"input_ids\"],\n",
    "            tokenized_text[\"token_type_ids\"], \n",
    "            position_ids,\n",
    "            tokenized_ref_text[\"input_ids\"],\n",
    "            tokenized_ref_text[\"token_type_ids\"], \n",
    "            ref_position_ids,\n",
    "            tokenized_text[\"attention_mask\"])   \n",
    "\n",
    "\n",
    "def prepare_input_embed(input_ids, token_type_ids, position_ids,\n",
    "                        ref_input_ids, ref_token_type_ids, ref_position_ids,\n",
    "                        attention_mask):\n",
    "    \"\"\"Construct input for the modified model\"\"\"\n",
    "    input_ids_embed = interpretable_embedding1.indices_to_embeddings(input_ids)\n",
    "    ref_input_ids_embed = interpretable_embedding1.indices_to_embeddings(ref_input_ids)\n",
    "    token_type_ids_embed = interpretable_embedding2.indices_to_embeddings(token_type_ids)\n",
    "    ref_token_type_ids_embed = interpretable_embedding2.indices_to_embeddings(ref_token_type_ids)\n",
    "    position_ids_embed = interpretable_embedding3.indices_to_embeddings(position_ids)\n",
    "    ref_position_ids_embed = interpretable_embedding3.indices_to_embeddings(ref_position_ids)\n",
    "    \n",
    "    return (input_ids_embed, token_type_ids_embed, position_ids_embed, \n",
    "            ref_input_ids_embed, ref_token_type_ids_embed, \n",
    "            ref_position_ids_embed, attention_mask)\n",
    "\n",
    "\n",
    "def get_input_data(text):\n",
    "    input_data = place_on_device(*prepare_input(text))\n",
    "    input_data_embed = prepare_input_embed(*input_data)   \n",
    "    return input_data, input_data_embed \n",
    "\n",
    "\n",
    "def place_on_device(*tensors):\n",
    "    tensors_device = []\n",
    "    for t in tensors:\n",
    "        tensors_device.append(t.to(device))\n",
    "    return tuple(tensors_device)  \n",
    "\n",
    "\n",
    "def ig_attribute(ig, class_index, input_data_embed):\n",
    "    return ig.attribute(inputs=input_data_embed[0:3],\n",
    "                        baselines=input_data_embed[3:6],\n",
    "                        additional_forward_args=(input_data_embed[6]),\n",
    "                        target = class_index,\n",
    "                        return_convergence_delta=True,\n",
    "                        n_steps=200)\n",
    "    \n",
    "\n",
    "def lig_attribute(lig, class_index, input_data):\n",
    "    return lig.attribute(\n",
    "        inputs=input_data[0], baselines=input_data[3],\n",
    "        additional_forward_args=(input_data[1], input_data[2], input_data[6]),\n",
    "        return_convergence_delta=True, target=class_index, n_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a78ee906",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(predict_forward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e6f18e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<captum.attr._core.integrated_gradients.IntegratedGradients at 0x7f87b6274be0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f557d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f1fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbff22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7581d289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831daec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c596c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14213091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2718511b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model's input embeddings:\n",
      " InterpretableEmbeddingBase(\n",
      "  (embedding): Embedding(30522, 128, padding_idx=0)\n",
      ")\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 # Compute attributions for both target classes</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 # class 0 (negative)</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>23 attributions_0, approximation_error_0 = ig_attribute(ig, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, input_data_embed)               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 # class 1 (positive)</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span>attributions_1, approximation_error_1 = ig_attribute(ig, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, input_data_embed)               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">ig_attribute</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">86</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">83 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">84 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">85 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">ig_attribute</span>(ig, class_index, input_data_embed):                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>86 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> ig.attribute(inputs=input_data_embed[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>],                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">87 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>baselines=input_data_embed[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span>],                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">88 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>additional_forward_args=(input_data_embed[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span>]),                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">89 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>target = class_index,                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/captum/attr/_core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">integrated_gradients.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">278</span> in          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">attribute</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">275 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>expanded_target = _expand_target(target, n_steps)                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">276 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">277 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># grads: dim -&gt; (bsz * #steps x inputs[0].shape[1:], ...)</span>                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>278 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>grads = _batched_operator(                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">279 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gradient_func,                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">280 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>scaled_features_tpl,                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">281 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>input_additional_args,                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/captum/attr/_utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">batching.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">156</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_batched_operator</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">153 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">   </span><span style=\"color: #808000; text-decoration-color: #808000\">to inputs and additional forward arguments, and returning the concatenation</span>            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">154 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">   </span><span style=\"color: #808000; text-decoration-color: #808000\">of the results of each batch.</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">155 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>156 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>all_outputs = [                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">157 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>operator(                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">158 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>inputs=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">159 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>additional_forward_args=additional,                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/captum/attr/_utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">batching.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">157</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">154 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">   </span><span style=\"color: #808000; text-decoration-color: #808000\">of the results of each batch.</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">155 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">156 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>all_outputs = [                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>157 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>operator(                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">158 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>inputs=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">159 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>additional_forward_args=additional,                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">160 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>target_ind=target,                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/captum/attr/_utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">gradient.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">96</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">compute_gradients</span>    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.autograd.set_grad_enabled(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>):                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># runs forward pass</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 96 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>outputs = _run_forward(forward_fn, inputs, target_ind, additional_forward_args)    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 97 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> outputs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>].numel() == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, (                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 98 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Target not provided when necessary, cannot\"</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #808000; text-decoration-color: #808000\">\" take gradient with respect to multiple outputs.\"</span>                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/captum/attr/_utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">common.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">500</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_run_forward</span>          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">497 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>inputs = _format_input(inputs)                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>additional_forward_args = _format_additional_forward_args(additional_forward_args)     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>500 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>output = forward_func(                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">501 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>*(*inputs, *additional_forward_args)                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> additional_forward_args <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> inputs                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict_forward_func</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">28</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict_forward_func</span>(input_ids, token_type_ids=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, position_ids=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, attention_ma    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Function passed to ig constructors\"\"\"</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>28 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> model(input_ids=input_ids,                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>token_type_ids=token_type_ids,                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>position_ids=position_ids,                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>attention_mask=attention_mask)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/models/electra/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_electra.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1004</span> in   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1001 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">      </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1002 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1003 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1004 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>discriminator_hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.electra(                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1005 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1006 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1007 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>token_type_ids=token_type_ids,                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/models/electra/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_electra.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">877</span> in    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 874 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 875 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"You have to specify either input_ids or inputs_embeds\"</span>)     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 876 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 877 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>batch_size, seq_length = input_shape                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 878 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>device = input_ids.device <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> input_ids <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> inputs_embeds.device      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 879 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 880 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># past_key_values_length</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>too many values to unpack <span style=\"font-weight: bold\">(</span>expected <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m23\u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m20 \u001b[0m                                                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m# Compute attributions for both target classes\u001b[0m                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m# class 0 (negative)\u001b[0m                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m23 attributions_0, approximation_error_0 = ig_attribute(ig, \u001b[94m0\u001b[0m, input_data_embed)               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m# class 1 (positive)\u001b[0m                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m25 \u001b[0mattributions_1, approximation_error_1 = ig_attribute(ig, \u001b[94m1\u001b[0m, input_data_embed)               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m26 \u001b[0m                                                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mig_attribute\u001b[0m:\u001b[94m86\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m83 \u001b[0m                                                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m84 \u001b[0m                                                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m85 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mig_attribute\u001b[0m(ig, class_index, input_data_embed):                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m86 \u001b[2m   \u001b[0m\u001b[94mreturn\u001b[0m ig.attribute(inputs=input_data_embed[\u001b[94m0\u001b[0m:\u001b[94m3\u001b[0m],                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m87 \u001b[0m\u001b[2m                  \u001b[0mbaselines=input_data_embed[\u001b[94m3\u001b[0m:\u001b[94m6\u001b[0m],                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m88 \u001b[0m\u001b[2m                  \u001b[0madditional_forward_args=(input_data_embed[\u001b[94m6\u001b[0m]),                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m89 \u001b[0m\u001b[2m                  \u001b[0mtarget = class_index,                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/captum/attr/_core/\u001b[0m\u001b[1;33mintegrated_gradients.py\u001b[0m:\u001b[94m278\u001b[0m in          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mattribute\u001b[0m                                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m275 \u001b[0m\u001b[2m      \u001b[0mexpanded_target = _expand_target(target, n_steps)                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m276 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m277 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[0m                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m278 \u001b[2m      \u001b[0mgrads = _batched_operator(                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m279 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mself\u001b[0m.gradient_func,                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m280 \u001b[0m\u001b[2m         \u001b[0mscaled_features_tpl,                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m281 \u001b[0m\u001b[2m         \u001b[0minput_additional_args,                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/captum/attr/_utils/\u001b[0m\u001b[1;33mbatching.py\u001b[0m:\u001b[94m156\u001b[0m in \u001b[92m_batched_operator\u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m153 \u001b[0m\u001b[2;33m   \u001b[0m\u001b[33mto inputs and additional forward arguments, and returning the concatenation\u001b[0m            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m154 \u001b[0m\u001b[2;33m   \u001b[0m\u001b[33mof the results of each batch.\u001b[0m                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m155 \u001b[0m\u001b[2;33m   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m156 \u001b[2m   \u001b[0mall_outputs = [                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m157 \u001b[0m\u001b[2m      \u001b[0moperator(                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m158 \u001b[0m\u001b[2m         \u001b[0minputs=\u001b[96minput\u001b[0m,                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m159 \u001b[0m\u001b[2m         \u001b[0madditional_forward_args=additional,                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/captum/attr/_utils/\u001b[0m\u001b[1;33mbatching.py\u001b[0m:\u001b[94m157\u001b[0m in \u001b[92m<listcomp>\u001b[0m          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m154 \u001b[0m\u001b[2;33m   \u001b[0m\u001b[33mof the results of each batch.\u001b[0m                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m155 \u001b[0m\u001b[2;33m   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m   \u001b[0mall_outputs = [                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m157 \u001b[2m      \u001b[0moperator(                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m158 \u001b[0m\u001b[2m         \u001b[0minputs=\u001b[96minput\u001b[0m,                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m159 \u001b[0m\u001b[2m         \u001b[0madditional_forward_args=additional,                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m160 \u001b[0m\u001b[2m         \u001b[0mtarget_ind=target,                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/captum/attr/_utils/\u001b[0m\u001b[1;33mgradient.py\u001b[0m:\u001b[94m96\u001b[0m in \u001b[92mcompute_gradients\u001b[0m    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2;33m   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mwith\u001b[0m torch.autograd.set_grad_enabled(\u001b[94mTrue\u001b[0m):                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# runs forward pass\u001b[0m                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 96 \u001b[2m      \u001b[0moutputs = _run_forward(forward_fn, inputs, target_ind, additional_forward_args)    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m      \u001b[0m\u001b[94massert\u001b[0m outputs[\u001b[94m0\u001b[0m].numel() == \u001b[94m1\u001b[0m, (                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m         \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mTarget not provided when necessary, cannot\u001b[0m\u001b[33m\"\u001b[0m                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2m         \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m take gradient with respect to multiple outputs.\u001b[0m\u001b[33m\"\u001b[0m                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/captum/attr/_utils/\u001b[0m\u001b[1;33mcommon.py\u001b[0m:\u001b[94m500\u001b[0m in \u001b[92m_run_forward\u001b[0m          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m497 \u001b[0m\u001b[2m   \u001b[0minputs = _format_input(inputs)                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m498 \u001b[0m\u001b[2m   \u001b[0madditional_forward_args = _format_additional_forward_args(additional_forward_args)     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m499 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m500 \u001b[2m   \u001b[0moutput = forward_func(                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m501 \u001b[0m\u001b[2m      \u001b[0m*(*inputs, *additional_forward_args)                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m502 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m additional_forward_args \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m503 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m inputs                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mpredict_forward_func\u001b[0m:\u001b[94m28\u001b[0m                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m25 \u001b[0m                                                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m26 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mpredict_forward_func\u001b[0m(input_ids, token_type_ids=\u001b[94mNone\u001b[0m, position_ids=\u001b[94mNone\u001b[0m, attention_ma    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2;90m   \u001b[0m\u001b[33m\"\"\"Function passed to ig constructors\"\"\"\u001b[0m                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m28 \u001b[2m   \u001b[0m\u001b[94mreturn\u001b[0m model(input_ids=input_ids,                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m             \u001b[0mtoken_type_ids=token_type_ids,                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m             \u001b[0mposition_ids=position_ids,                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m             \u001b[0mattention_mask=attention_mask)[\u001b[94m0\u001b[0m]                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1130\u001b[0m in \u001b[92m_call_impl\u001b[0m             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1130 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m      \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/models/electra/\u001b[0m\u001b[1;33mmodeling_electra.py\u001b[0m:\u001b[94m1004\u001b[0m in   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1001 \u001b[0m\u001b[2;33m      \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1002 \u001b[0m\u001b[2m      \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1003 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1004 \u001b[2m      \u001b[0mdiscriminator_hidden_states = \u001b[96mself\u001b[0m.electra(                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1005 \u001b[0m\u001b[2m         \u001b[0minput_ids,                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1006 \u001b[0m\u001b[2m         \u001b[0mattention_mask=attention_mask,                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1007 \u001b[0m\u001b[2m         \u001b[0mtoken_type_ids=token_type_ids,                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1130\u001b[0m in \u001b[92m_call_impl\u001b[0m             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1130 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m      \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/models/electra/\u001b[0m\u001b[1;33mmodeling_electra.py\u001b[0m:\u001b[94m877\u001b[0m in    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 874 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 875 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mYou have to specify either input_ids or inputs_embeds\u001b[0m\u001b[33m\"\u001b[0m)     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 876 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 877 \u001b[2m      \u001b[0mbatch_size, seq_length = input_shape                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 878 \u001b[0m\u001b[2m      \u001b[0mdevice = input_ids.device \u001b[94mif\u001b[0m input_ids \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m inputs_embeds.device      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 879 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 880 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# past_key_values_length\u001b[0m                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mtoo many values to unpack \u001b[1m(\u001b[0mexpected \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure interpretable embedding layer \n",
    "print(\"Original model's input embeddings:\\n {}\\n\".\n",
    "      format(model.get_input_embeddings()))\n",
    "if not \\\n",
    "type(model.get_input_embeddings()).__name__ == \"InterpretableEmbeddingBase\":\n",
    "    interpretable_embedding1, interpretable_embedding2,\\\n",
    "    interpretable_embedding3 = configure_interpretable_embeddings()\n",
    "#     interpretable_embedding1 = model.electra.embeddings.word_embeddings\n",
    "#     interpretable_embedding2 = model.electra.embeddings.token_type_embeddings\n",
    "#     interpretable_embedding3 = model.electra.embeddings.position_embeddings\n",
    "\n",
    "\n",
    "# Prepare input\n",
    "input_data, input_data_embed = get_input_data(text)  \n",
    "\n",
    "# print(input_data)\n",
    "# print(input_data_embed)\n",
    "# print(len(input_data_embed))\n",
    "\n",
    "\n",
    "# Compute attributions for both target classes\n",
    "# class 0 (negative)\n",
    "attributions_0, approximation_error_0 = ig_attribute(ig, 0, input_data_embed)\n",
    "# class 1 (positive)\n",
    "attributions_1, approximation_error_1 = ig_attribute(ig, 1, input_data_embed)\n",
    "\n",
    "# Remove interpratable embedding layer used by ig attribution\n",
    "remove_interpretable_embeddings(interpretable_embedding1, \n",
    "                                interpretable_embedding2, \n",
    "                                interpretable_embedding3)\n",
    "print(\"\\nInput embeddings with interpretable layer removed:\\n {}\\n\"\n",
    ".format(model.get_input_embeddings()))\n",
    "\n",
    "print(\"\\nThe reference sample:\\n{}\".format(tokenizer.convert_ids_to_tokens(\n",
    "    input_data[3].clone().detach().to('cpu').numpy().squeeze())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc8a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce5f8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3dd7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227585d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cc5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f02847c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraForSequenceClassification(\n",
      "  (electra): ElectraModel(\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): ElectraClassificationHead(\n",
      "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_completeness(attributions_0, attributions_1):\n",
    "    input_ids, token_type_ids, position_ids, ref_input_ids,\\\n",
    "    ref_token_type_ids, ref_position_ids, attention_mask = input_data\n",
    "\n",
    "    # Prediction for the sample\n",
    "    scores = predict_forward_func(input_ids, token_type_ids,\n",
    "                                position_ids, attention_mask) \n",
    "    # Prediction for the baseline\n",
    "    ref_scores = predict_forward_func(ref_input_ids, ref_token_type_ids,\n",
    "                                    ref_position_ids, attention_mask)\n",
    "\n",
    "    # Put on cpu\n",
    "    if torch.is_tensor(attributions_0[0]):\n",
    "        attributions_0 = [x.clone().detach().to('cpu').numpy() \n",
    "        for x in attributions_0]\n",
    "    if torch.is_tensor(attributions_1[0]):\n",
    "        attributions_1 = [x.clone().detach().to('cpu').numpy() \n",
    "        for x in attributions_1]  \n",
    "    scores = scores.clone().detach().to('cpu').numpy().squeeze()\n",
    "    ref_scores = ref_scores.clone().detach().to('cpu').numpy().squeeze()    \n",
    "\n",
    "    # How prediction for the sample differs from baseline prediction  \n",
    "    diff_from_baseline = scores - ref_scores\n",
    "\n",
    "    # Sum of attributions\n",
    "    attributions_sum0 = [x.sum() for x in attributions_0]\n",
    "    attributions_sum1 = [x.sum() for x in attributions_1]\n",
    "    attributions_sum = [sum(attributions_sum0), sum(attributions_sum1)]\n",
    "\n",
    "    # Difference from the baseline output for both classes\n",
    "    diff = diff_from_baseline - attributions_sum\n",
    "\n",
    "    # Find out which layers contribute to the score \n",
    "    print(\"Class 0: input tokens attr. sum: {}\".format(attributions_sum0[0]))\n",
    "    print(\"Classs 0: token type attr. sum: {}\".format(attributions_0[1].sum()))\n",
    "    print(\"Class 0: position ids attr. sum: {}\".format(attributions_0[2].sum()))\n",
    "    print(\"Class 1: input tokens attr. sum: {}\".format(attributions_1[0].sum()))\n",
    "    print(\"Classs 1: token type attr. sum: {}\".format(attributions_1[1].sum()))\n",
    "    print(\"Class 1: position ids attr. sum: {}\".format(attributions_1[2].sum()))\n",
    "\n",
    "    # Compare sum of attributions with the difference from baseline prediction\n",
    "    print(\"\\nPrediction for sample: {}\".format(scores))\n",
    "    print(\"Prediction for baseline: {}\".format(ref_scores))\n",
    "    print(\"Difference from baseline: {}\".format(diff_from_baseline))\n",
    "    print(\"Sum of attributions: {}\".format(attributions_sum))\n",
    "    print(\"\\nClass 0:\\n score: {}\\n reference score: {}\\\n",
    "    \\n difference from ref.: {}\\n sum of attributions:  {}\\\n",
    "    \\n difference from reference - attributions: {}\".\\\n",
    "    format(scores[0], ref_scores[0], diff_from_baseline[0], \n",
    "            attributions_sum[0], diff[0]))\n",
    "    print(\"\\nClass 1:\\n score: {}\\n reference score: {}\\\n",
    "    \\n difference from ref.: {}\\n sum of attributions:  {}\\\n",
    "    \\n difference from reference - attributions: {}\".\\\n",
    "    format(scores[1], ref_scores[1], diff_from_baseline[1], \n",
    "            attributions_sum[1], diff[1]))\n",
    "    \n",
    "    return attributions_0, attributions_1\n",
    "    \n",
    "    \n",
    "attributions_0, attributions_1 = check_completeness(attributions_0,\n",
    "                                                    attributions_1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429b830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5a887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285ff00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e409a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9538b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8114f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5c9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f14fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38194e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
